python3 -m torch.distributed.run --nproc_per_node 1 chat.py --ckpt_dir ./llama-2-7b-chat/ --tokenizer_path tokenizer.model --max_seq_len 512 --max_gen_len 40 --max_batch_size 3
python3 -m torch.distributed.run --nproc_per_node 1 app.py --ckpt_dir ./llama-2-7b-chat/ --tokenizer_path tokenizer.model --max_seq_len 512 --max_gen_len 40 --max_batch_size 3

ifconfig --> <IP>
netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=5000 connectaddress=<IP> connectport=5000
netsh interface portproxy show v4tov4

--> Connection:
    Use 192.168.... to connect wsl

--> Use screen to run programs parallely:
    screen -S <screen-name>
    screen -ls
    screen -r
    screen -XS <session-id> quit
    ctrl + a + d --> detach anytime

--> Expose api to internet (temporary)
    ngrok http 5000
